{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e56784db",
   "metadata": {},
   "source": [
    "## Importaciones necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a8ea78af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "from typing import Annotated\n",
    "from openai import AsyncOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from semantic_kernel.agents import ChatCompletionAgent, ChatHistoryAgentThread\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.contents import FunctionCallContent, FunctionResultContent, StreamingTextContent\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bd745c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "client = AsyncOpenAI(\n",
    "    api_key=os.environ.get(\"GITHUB_TOKEN\"), \n",
    "    base_url=\"https://models.inference.ai.azure.com/\",\n",
    ")\n",
    "\n",
    "# Create an AI Service that will be used by the `ChatCompletionAgent`\n",
    "chat_completion_service = OpenAIChatCompletion(\n",
    "    ai_model_id=\"gpt-4o-mini\",\n",
    "    async_client=client,\n",
    "    #api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "71022e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key detectada: ✅\n"
     ]
    }
   ],
   "source": [
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"API Key detectada:\", \"✅\" if api_key else \"❌ NO detectada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f9797b",
   "metadata": {},
   "source": [
    "## Clase principal del plugin\n",
    "\n",
    "\n",
    "*\n",
    "- Filtra los datos por edad.\n",
    "- Calcula los promedios de presión sistólica y diastólica.\n",
    "- Devuelve un string con los valores.\n",
    "\n",
    "*\n",
    "- Permite consultar toda la info de un paciente usando el número de fila (índice).\n",
    "- Verifica que el índice esté dentro del rango."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e8c57f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from BloodPressurePlugin import BloodPressurePlugin\n",
    "\n",
    "class BloodPressurePlugin:\n",
    "    def __init__(self):\n",
    "        self.df = pd.read_csv(\"Database/BloodPressuredataset.csv\")\n",
    "\n",
    "    @kernel_function(description=\"Devuelve el promedio de presión para una edad específica\")\n",
    "    def get_average_pressure_by_age(self, age: Annotated[int, \"Edad del paciente\"]) -> str:\n",
    "        filtered_df = self.df[self.df[\"Edad\"] == age]\n",
    "        if filtered_df.empty:\n",
    "            return f\"No se encontraron datos para edad {age}.\"\n",
    "        sys = filtered_df[\"Presion_Sistolica\"].mean()\n",
    "        dia = filtered_df[\"Presion_Diastolica\"].mean()\n",
    "        return f\"Presión promedio para {age} años: {sys:.2f}/{dia:.2f} mmHg\"\n",
    "\n",
    "    @kernel_function(description=\"Muestra la información completa de un paciente por índice\")\n",
    "    def get_patient_info_by_index(self, index: Annotated[int, \"Índice del paciente\"]) -> str:\n",
    "        if index < 0 or index >= len(self.df):\n",
    "            return f\"Índice fuera de rango. Hay {len(self.df)} pacientes.\"\n",
    "        return self.df.iloc[index].to_string()\n",
    "\n",
    "    @kernel_function(description=\"Paciente con la presión total más alta\")\n",
    "    def get_highest_pressure_patient(self) -> str:\n",
    "        self.df[\"Presion_Total\"] = self.df[\"Presion_Sistolica\"] + self.df[\"Presion_Diastolica\"]\n",
    "        idx = self.df[\"Presion_Total\"].idxmax()\n",
    "        return self.df.iloc[idx].to_string()\n",
    "\n",
    "    @kernel_function(description=\"Promedio de glucosa en todos los pacientes\")\n",
    "    def get_average_glucose(self) -> str:\n",
    "        avg = self.df[\"Glucosa\"].mean()\n",
    "        return f\"Promedio de glucosa: {avg:.2f} mg/dL\"\n",
    "\n",
    "    @kernel_function(description=\"Paciente con glucosa más alta\")\n",
    "    def get_max_glucose_patient(self) -> str:\n",
    "        idx = self.df[\"Glucosa\"].idxmax()\n",
    "        return self.df.iloc[idx].to_string()\n",
    "\n",
    "    @kernel_function(description=\"Devuelve pacientes con nivel de estrés alto\")\n",
    "    def get_high_stress_patients(self) -> str:\n",
    "        high_stress = self.df[self.df[\"Nivel_Estres\"] > 7]  # supón que >7 es alto\n",
    "        if high_stress.empty:\n",
    "            return \"No hay pacientes con nivel de estrés alto.\"\n",
    "        return high_stress[[\"Edad\", \"Nivel_Estres\", \"Frecuencia_Cardiaca\"]].to_string(index=False)\n",
    "\n",
    "    @kernel_function(description=\"Resumen general de salud para todos los pacientes\")\n",
    "    def get_general_health_summary(self) -> str:\n",
    "        summary = self.df.describe().to_string()\n",
    "        return f\"Resumen estadístico general:\\n{summary}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379c3200",
   "metadata": {},
   "source": [
    "## Funciones decoradas con @kernel_function\n",
    "Estas funciones son las que el agente puede usar para responder preguntas. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fb1dd8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kernel_function(description=\"Devuelve el promedio de presión para una edad específica\")\n",
    "def get_average_pressure_by_age(self, age: Annotated[int, \"Edad del paciente\"]) -> str:\n",
    "    filtered_df = self.df[self.df[\"Edad\"] == age]\n",
    "    if filtered_df.empty:\n",
    "        return f\"No se encontraron datos para edad {age}.\"\n",
    "    systolic_avg = filtered_df[\"Presion_Sistolica\"].mean()\n",
    "    diastolic_avg = filtered_df[\"Presion_Diastolica\"].mean()\n",
    "    return f\"Presión promedio para edad {age}: {systolic_avg:.2f}/{diastolic_avg:.2f} mmHg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e5bedecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kernel_function(description=\"Muestra la información completa de un paciente por índice\")\n",
    "def get_patient_info_by_index(self, index: Annotated[int, \"Índice del paciente\"]) -> str:\n",
    "    if index < 0 or index >= len(self.df):\n",
    "        return f\"Índice fuera de rango. Hay {len(self.df)} pacientes.\"\n",
    "    patient = self.df.iloc[index]\n",
    "    return patient.to_string()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "10d14a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kernel_function(description=\"Devuelve el paciente con la presión más alta\")\n",
    "def get_highest_pressure_patient(self) -> str:\n",
    "    self.df[\"Presion_Total\"] = self.df[\"Presion_Sistolica\"] + self.df[\"Presion_Diastolica\"]\n",
    "    max_index = self.df[\"Presion_Total\"].idxmax()\n",
    "    return self.df.iloc[max_index].to_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffc7f8d",
   "metadata": {},
   "source": [
    "* Responder preguntas como “¿Cuál es la presión promedio para una persona de 45 años?”\n",
    "\n",
    "* Mostrar información específica de un paciente por índice\n",
    "\n",
    "* Detectar automáticamente al paciente más crítico con mayor presión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57570487",
   "metadata": {},
   "source": [
    "### Conectar tu plugin BloodPressurePlugin al agente VitalMind\n",
    "Asumiendo que ya tenemos el agente VitalMind declarado así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5c3fbc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ChatCompletionAgent(\n",
    "    service=chat_completion_service,\n",
    "    plugins=[BloodPressurePlugin()],\n",
    "    name=\"VitalMind\",\n",
    "    instructions=\"Soy Eres un asistente de salud que ayuda a interpretar datos clínicos como presión arterial, glucosa, estrés y más, ofreciendo respuestas útiles, claras y seguras.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bade023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar el servicio de OpenAI (recuerda que tu API Key debe estar bien configurada)\n",
    "kernel = kernel_function()\n",
    "service = OpenAIChatCompletion(service_id=\"openai\", ai_model_id=\"gpt-4o-mini\", ) #api_key=\"OPENAI_API_KEY\"\n",
    "agent = ChatCompletionAgent(\n",
    "    service=service,\n",
    "    plugins=[BloodPressurePlugin()],\n",
    "    name=\"VitalMind\",\n",
    "    instructions=(\n",
    "        \"Eres un agente médico llamado VitalMind. Tu objetivo es analizar datos clínicos \"\n",
    "        \"como presión arterial, glucosa, estrés y más. Debes responder de manera clara, \"\n",
    "        \"útil y apropiada para personas preocupadas por su salud.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d3785d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Usuario: Tengo 75 años y me han subido los niveles de glucosa, ¿qué puedo hacer?\n",
      "\n"
     ]
    },
    {
     "ename": "ServiceResponseException",
     "evalue": "(\"<class 'semantic_kernel.connectors.ai.open_ai.services.open_ai_chat_completion.OpenAIChatCompletion'> service failed to complete the prompt\", RateLimitError(\"Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_handler.py:87\u001b[39m, in \u001b[36mOpenAIHandler._send_completion_request\u001b[39m\u001b[34m(self, settings)\u001b[39m\n\u001b[32m     86\u001b[39m         settings_dict.pop(\u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.chat.completions.create(**settings_dict)\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2032\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2031\u001b[39m validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2032\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2033\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2034\u001b[39m     body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2035\u001b[39m         {\n\u001b[32m   2036\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2037\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2038\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2039\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2040\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2041\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2042\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2043\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2044\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2045\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2046\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2047\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2048\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2049\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2050\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2051\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2052\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2053\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2054\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2055\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2056\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2057\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2058\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2059\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2060\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2061\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2062\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2063\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2064\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2065\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2066\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2067\u001b[39m         },\n\u001b[32m   2068\u001b[39m         completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2069\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2070\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2071\u001b[39m     ),\n\u001b[32m   2072\u001b[39m     options=make_request_options(\n\u001b[32m   2073\u001b[39m         extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2074\u001b[39m     ),\n\u001b[32m   2075\u001b[39m     cast_to=ChatCompletion,\n\u001b[32m   2076\u001b[39m     stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2077\u001b[39m     stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2078\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1805\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1802\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1803\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1804\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1805\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1495\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[39m\n\u001b[32m   1493\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1495\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m   1496\u001b[39m     cast_to=cast_to,\n\u001b[32m   1497\u001b[39m     options=options,\n\u001b[32m   1498\u001b[39m     stream=stream,\n\u001b[32m   1499\u001b[39m     stream_cls=stream_cls,\n\u001b[32m   1500\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1501\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1585\u001b[39m, in \u001b[36mAsyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[39m\n\u001b[32m   1584\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m err.response.aclose()\n\u001b[32m-> \u001b[39m\u001b[32m1585\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retry_request(\n\u001b[32m   1586\u001b[39m         input_options,\n\u001b[32m   1587\u001b[39m         cast_to,\n\u001b[32m   1588\u001b[39m         retries_taken=retries_taken,\n\u001b[32m   1589\u001b[39m         response_headers=err.response.headers,\n\u001b[32m   1590\u001b[39m         stream=stream,\n\u001b[32m   1591\u001b[39m         stream_cls=stream_cls,\n\u001b[32m   1592\u001b[39m     )\n\u001b[32m   1594\u001b[39m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[32m   1595\u001b[39m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1632\u001b[39m, in \u001b[36mAsyncAPIClient._retry_request\u001b[39m\u001b[34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[39m\n\u001b[32m   1630\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m anyio.sleep(timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1632\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m   1633\u001b[39m     options=options,\n\u001b[32m   1634\u001b[39m     cast_to=cast_to,\n\u001b[32m   1635\u001b[39m     retries_taken=retries_taken + \u001b[32m1\u001b[39m,\n\u001b[32m   1636\u001b[39m     stream=stream,\n\u001b[32m   1637\u001b[39m     stream_cls=stream_cls,\n\u001b[32m   1638\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1585\u001b[39m, in \u001b[36mAsyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[39m\n\u001b[32m   1584\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m err.response.aclose()\n\u001b[32m-> \u001b[39m\u001b[32m1585\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retry_request(\n\u001b[32m   1586\u001b[39m         input_options,\n\u001b[32m   1587\u001b[39m         cast_to,\n\u001b[32m   1588\u001b[39m         retries_taken=retries_taken,\n\u001b[32m   1589\u001b[39m         response_headers=err.response.headers,\n\u001b[32m   1590\u001b[39m         stream=stream,\n\u001b[32m   1591\u001b[39m         stream_cls=stream_cls,\n\u001b[32m   1592\u001b[39m     )\n\u001b[32m   1594\u001b[39m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[32m   1595\u001b[39m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1632\u001b[39m, in \u001b[36mAsyncAPIClient._retry_request\u001b[39m\u001b[34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[39m\n\u001b[32m   1630\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m anyio.sleep(timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1632\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m   1633\u001b[39m     options=options,\n\u001b[32m   1634\u001b[39m     cast_to=cast_to,\n\u001b[32m   1635\u001b[39m     retries_taken=retries_taken + \u001b[32m1\u001b[39m,\n\u001b[32m   1636\u001b[39m     stream=stream,\n\u001b[32m   1637\u001b[39m     stream_cls=stream_cls,\n\u001b[32m   1638\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1600\u001b[39m, in \u001b[36mAsyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[39m\n\u001b[32m   1599\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1600\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1603\u001b[39m     cast_to=cast_to,\n\u001b[32m   1604\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1608\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1609\u001b[39m )\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mServiceResponseException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[120]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m thread.delete() \u001b[38;5;28;01mif\u001b[39;00m thread \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Ejecutar el asistente\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[120]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m# Usuario: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m first_chunk = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m agent.invoke_stream(messages=user_input, thread=thread):\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m first_chunk:\n\u001b[32m     16\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m# VitalMind: \u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\semantic_kernel\\utils\\telemetry\\agent_diagnostics\\decorators.py:39\u001b[39m, in \u001b[36mtrace_agent_invocation.<locals>.wrapper_decorator\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m agent.description:\n\u001b[32m     37\u001b[39m     span.set_attribute(gen_ai_attributes.AGENT_DESCRIPTION, agent.description)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m invoke_func(*args, **kwargs):\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\semantic_kernel\\agents\\chat_completion\\chat_completion_agent.py:419\u001b[39m, in \u001b[36mChatCompletionAgent.invoke_stream\u001b[39m\u001b[34m(self, messages, thread, on_intermediate_message, arguments, kernel, **kwargs)\u001b[39m\n\u001b[32m    417\u001b[39m role = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    418\u001b[39m response_builder: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response_list \u001b[38;5;129;01min\u001b[39;00m responses:\n\u001b[32m    420\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m response_list:\n\u001b[32m    421\u001b[39m         role = response.role\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\chat_completion_client_base.py:261\u001b[39m, in \u001b[36mChatCompletionClientBase.get_streaming_chat_message_contents\u001b[39m\u001b[34m(self, chat_history, settings, **kwargs)\u001b[39m\n\u001b[32m    259\u001b[39m all_messages: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mStreamingChatMessageContent\u001b[39m\u001b[33m\"\u001b[39m] = []\n\u001b[32m    260\u001b[39m function_call_returned = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m messages \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inner_get_streaming_chat_message_contents(\n\u001b[32m    262\u001b[39m     chat_history, settings, request_index\n\u001b[32m    263\u001b[39m ):\n\u001b[32m    264\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m messages:\n\u001b[32m    265\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\semantic_kernel\\utils\\telemetry\\model_diagnostics\\decorators.py:165\u001b[39m, in \u001b[36mtrace_streaming_chat_completion.<locals>.inner_trace_streaming_chat_completion.<locals>.wrapper_decorator\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(completion_func)\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper_decorator\u001b[39m(\n\u001b[32m    161\u001b[39m     *args: Any, **kwargs: Any\n\u001b[32m    162\u001b[39m ) -> AsyncGenerator[\u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mStreamingChatMessageContent\u001b[39m\u001b[33m\"\u001b[39m], Any]:\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m are_model_diagnostics_enabled():\n\u001b[32m    164\u001b[39m         \u001b[38;5;66;03m# If model diagnostics are not enabled, just return the completion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m streaming_chat_message_contents \u001b[38;5;129;01min\u001b[39;00m completion_func(*args, **kwargs):\n\u001b[32m    166\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m streaming_chat_message_contents\n\u001b[32m    167\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_chat_completion_base.py:110\u001b[39m, in \u001b[36mOpenAIChatCompletionBase._inner_get_streaming_chat_message_contents\u001b[39m\u001b[34m(self, chat_history, settings, function_invoke_attempt)\u001b[39m\n\u001b[32m    107\u001b[39m settings.messages = \u001b[38;5;28mself\u001b[39m._prepare_chat_history_for_request(chat_history)\n\u001b[32m    108\u001b[39m settings.ai_model_id = settings.ai_model_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ai_model_id\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_request(settings)\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, AsyncStream):\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceInvalidResponseError(\u001b[33m\"\u001b[39m\u001b[33mExpected an AsyncStream[ChatCompletionChunk] response.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_handler.py:59\u001b[39m, in \u001b[36mOpenAIHandler._send_request\u001b[39m\u001b[34m(self, settings)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ai_model_type == OpenAIModelTypes.TEXT \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ai_model_type == OpenAIModelTypes.CHAT:\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(settings, OpenAIPromptExecutionSettings)  \u001b[38;5;66;03m# nosec\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_completion_request(settings)\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ai_model_type == OpenAIModelTypes.EMBEDDING:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(settings, OpenAIEmbeddingPromptExecutionSettings)  \u001b[38;5;66;03m# nosec\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_handler.py:104\u001b[39m, in \u001b[36mOpenAIHandler._send_completion_request\u001b[39m\u001b[34m(self, settings)\u001b[39m\n\u001b[32m     99\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceResponseException(\n\u001b[32m    100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m service failed to complete the prompt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    101\u001b[39m         ex,\n\u001b[32m    102\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceResponseException(\n\u001b[32m    105\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m service failed to complete the prompt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    106\u001b[39m         ex,\n\u001b[32m    107\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n",
      "\u001b[31mServiceResponseException\u001b[39m: (\"<class 'semantic_kernel.connectors.ai.open_ai.services.open_ai_chat_completion.OpenAIChatCompletion'> service failed to complete the prompt\", RateLimitError(\"Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\"))"
     ]
    }
   ],
   "source": [
    "# Función para ejecutar preguntas al agente\n",
    "async def main():\n",
    "    thread: ChatHistoryAgentThread | None = None\n",
    "\n",
    "    user_inputs = [\n",
    "        \"Tengo 75 años y me han subido los niveles de glucosa, ¿qué puedo hacer?\",\n",
    "        \"Mi padre tiene 82 años y a veces se le olvida tomar sus medicamentos, ¿cómo puedo ayudarlo?\",\n",
    "\n",
    "    ]\n",
    "\n",
    "    for user_input in user_inputs:\n",
    "        print(f\"# Usuario: {user_input}\\n\")\n",
    "        first_chunk = True\n",
    "        async for response in agent.invoke_stream(messages=user_input, thread=thread):\n",
    "            if first_chunk:\n",
    "                print(f\"# VitalMind: \", end=\"\", flush=True)\n",
    "                first_chunk = False\n",
    "            print(f\"{response}\", end=\"\", flush=True)\n",
    "            thread = response.thread\n",
    "        print()\n",
    "\n",
    "    await thread.delete() if thread else None\n",
    "\n",
    "# Ejecutar el asistente\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e7107fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from typing import Annotated\n",
    "from openai import AsyncOpenAI\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from semantic_kernel.agents import ChatCompletionAgent, ChatHistoryAgentThread\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "from semantic_kernel.functions import kernel_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f7f243d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas del archivo:\n",
      "   Edad     Género   IMC  Glucosa  Colesterol  Frecuencia_Cardiaca  \\\n",
      "0    56   Femenino  16.3    104.4       203.7                   86   \n",
      "1    69  Masculino  20.8    113.4       192.9                   93   \n",
      "2    46   Femenino  25.7     90.8       147.1                   80   \n",
      "3    32   Femenino  26.3     95.3       159.5                   89   \n",
      "4    60  Masculino  28.0     77.8       206.6                   92   \n",
      "\n",
      "  Nivel_Estres  Presion_Sistolica  Presion_Diastolica  \n",
      "0         Bajo              143.9                95.5  \n",
      "1        Medio              113.5                70.2  \n",
      "2         Alto              109.0                67.6  \n",
      "3         Bajo              111.3                71.8  \n",
      "4         Bajo              118.0                99.5  \n",
      "\n",
      "Información de las columnas:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50 entries, 0 to 49\n",
      "Data columns (total 9 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Edad                 50 non-null     int64  \n",
      " 1   Género               50 non-null     object \n",
      " 2   IMC                  50 non-null     float64\n",
      " 3   Glucosa              50 non-null     float64\n",
      " 4   Colesterol           50 non-null     float64\n",
      " 5   Frecuencia_Cardiaca  50 non-null     int64  \n",
      " 6   Nivel_Estres         50 non-null     object \n",
      " 7   Presion_Sistolica    50 non-null     float64\n",
      " 8   Presion_Diastolica   50 non-null     float64\n",
      "dtypes: float64(5), int64(2), object(2)\n",
      "memory usage: 3.6+ KB\n",
      "None\n",
      "\n",
      "Estadísticas descriptivas:\n",
      "            Edad        IMC     Glucosa  Colesterol  Frecuencia_Cardiaca  \\\n",
      "count  50.000000  50.000000   50.000000   50.000000            50.000000   \n",
      "mean   51.800000  25.424000   98.772000  184.374000            83.860000   \n",
      "std    18.538432   4.441206   15.249089   28.879622            10.722873   \n",
      "min    19.000000  16.300000   70.900000  122.000000            61.000000   \n",
      "25%    38.250000  21.450000   88.450000  164.350000            80.000000   \n",
      "50%    51.500000  26.050000   96.750000  184.550000            86.500000   \n",
      "75%    68.750000  27.825000  108.075000  204.300000            92.000000   \n",
      "max    79.000000  35.500000  144.200000  271.800000            99.000000   \n",
      "\n",
      "       Presion_Sistolica  Presion_Diastolica  \n",
      "count          50.000000           50.000000  \n",
      "mean          121.902000           80.360000  \n",
      "std            13.818185           10.946288  \n",
      "min            88.600000           61.300000  \n",
      "25%           112.975000           72.425000  \n",
      "50%           121.900000           79.750000  \n",
      "75%           130.125000           85.200000  \n",
      "max           153.700000          112.500000  \n"
     ]
    }
   ],
   "source": [
    "# Cargar el archivo CSV\n",
    "df = pd.read_csv('Database/BloodPressuredataset.csv')\n",
    "\n",
    "# Mostrar las primeras 5 filas\n",
    "print(\"Primeras filas del archivo:\")\n",
    "print(df.head())\n",
    "\n",
    "# Mostrar información general de las columnas\n",
    "print(\"\\nInformación de las columnas:\")\n",
    "print(df.info())\n",
    "\n",
    "# Estadísticas básicas de las columnas numéricas\n",
    "print(\"\\nEstadísticas descriptivas:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48fee414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Annotated\n",
    "\n",
    "\n",
    "class HealthDataPlugin:\n",
    "    \"\"\"Proporciona funciones basadas en datos médicos de pacientes.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_path: str):\n",
    "        # Cargar el dataset de pacientes desde un archivo CSV\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "\n",
    "    @kernel_function(description=\"Devuelve el promedio de presión sistólica de todos los pacientes.\")\n",
    "    def get_avg_systolic_pressure(self) -> Annotated[float, \"Promedio de presión sistólica\"]:\n",
    "        return float(self.df[\"Presion_Sistolica\"].mean())\n",
    "\n",
    "    @kernel_function(description=\"Devuelve la cantidad de pacientes con IMC mayor a 30 (obesidad).\")\n",
    "    def get_obese_patient_count(self) -> Annotated[int, \"Cantidad de pacientes con IMC alto\"]:\n",
    "        return int((self.df[\"IMC\"] > 30).sum())\n",
    "\n",
    "    @kernel_function(description=\"Devuelve la edad promedio de los pacientes.\")\n",
    "    def get_avg_age(self) -> Annotated[float, \"Edad promedio de los pacientes\"]:\n",
    "        return float(self.df[\"Edad\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e4ae593",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "client = AsyncOpenAI(\n",
    "    api_key=os.environ.get(\"GITHUB_TOKEN\"), \n",
    "    base_url=\"https://models.inference.ai.azure.com/\",\n",
    ")\n",
    "\n",
    "# Create an AI Service that will be used by the `ChatCompletionAgent`\n",
    "chat_completion_service = OpenAIChatCompletion(\n",
    "    ai_model_id=\"gpt-4o-mini\",\n",
    "    async_client=client,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "856fad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "health_plugin = HealthDataPlugin(\"Database/BloodPressuredataset.csv\")\n",
    "\n",
    "agent = ChatCompletionAgent(\n",
    "    service=chat_completion_service,\n",
    "    plugins=[health_plugin],\n",
    "    name=\"VitalMind\",\n",
    "    instructions=(\n",
    "        \"Eres VitalMind, un agente experto en salud. \"\n",
    "        \"Puedes acceder a información médica real de pacientes desde un archivo CSV. \"\n",
    "        \"Responde de manera clara, empática y útil a las preguntas del usuario.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45a0c06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: Tengo 75 años y me han subido los niveles de glucosa, ¿qué puedo hacer?\n",
      "\n"
     ]
    },
    {
     "ename": "ServiceResponseException",
     "evalue": "(\"<class 'semantic_kernel.connectors.ai.open_ai.services.open_ai_chat_completion.OpenAIChatCompletion'> service failed to complete the prompt\", AuthenticationError(\"Error code: 401 - {'error': {'code': 'unauthorized', 'message': 'Bad credentials', 'details': 'Bad credentials'}}\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_handler.py:87\u001b[39m, in \u001b[36mOpenAIHandler._send_completion_request\u001b[39m\u001b[34m(self, settings)\u001b[39m\n\u001b[32m     86\u001b[39m         settings_dict.pop(\u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.chat.completions.create(**settings_dict)\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2032\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2031\u001b[39m validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2032\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2033\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2034\u001b[39m     body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2035\u001b[39m         {\n\u001b[32m   2036\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2037\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2038\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2039\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2040\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2041\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2042\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2043\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2044\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2045\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2046\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2047\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2048\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2049\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2050\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2051\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2052\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2053\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2054\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2055\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2056\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2057\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2058\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2059\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2060\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2061\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2062\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2063\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2064\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2065\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2066\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2067\u001b[39m         },\n\u001b[32m   2068\u001b[39m         completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2069\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2070\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2071\u001b[39m     ),\n\u001b[32m   2072\u001b[39m     options=make_request_options(\n\u001b[32m   2073\u001b[39m         extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2074\u001b[39m     ),\n\u001b[32m   2075\u001b[39m     cast_to=ChatCompletion,\n\u001b[32m   2076\u001b[39m     stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2077\u001b[39m     stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2078\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1805\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1802\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1803\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1804\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1805\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1495\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[39m\n\u001b[32m   1493\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1495\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m   1496\u001b[39m     cast_to=cast_to,\n\u001b[32m   1497\u001b[39m     options=options,\n\u001b[32m   1498\u001b[39m     stream=stream,\n\u001b[32m   1499\u001b[39m     stream_cls=stream_cls,\n\u001b[32m   1500\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1501\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1600\u001b[39m, in \u001b[36mAsyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[39m\n\u001b[32m   1599\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1600\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1603\u001b[39m     cast_to=cast_to,\n\u001b[32m   1604\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1608\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1609\u001b[39m )\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'code': 'unauthorized', 'message': 'Bad credentials', 'details': 'Bad credentials'}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mServiceResponseException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# Clean up the thread\u001b[39;00m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m thread.delete() \u001b[38;5;28;01mif\u001b[39;00m thread \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m# User: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m first_chunk = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m agent.invoke_stream(\n\u001b[32m     16\u001b[39m     messages=user_input, thread=thread,\n\u001b[32m     17\u001b[39m ):\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# 5. Print the response\u001b[39;00m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m first_chunk:\n\u001b[32m     20\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m# \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\semantic_kernel\\utils\\telemetry\\agent_diagnostics\\decorators.py:39\u001b[39m, in \u001b[36mtrace_agent_invocation.<locals>.wrapper_decorator\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m agent.description:\n\u001b[32m     37\u001b[39m     span.set_attribute(gen_ai_attributes.AGENT_DESCRIPTION, agent.description)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m invoke_func(*args, **kwargs):\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\semantic_kernel\\agents\\chat_completion\\chat_completion_agent.py:419\u001b[39m, in \u001b[36mChatCompletionAgent.invoke_stream\u001b[39m\u001b[34m(self, messages, thread, on_intermediate_message, arguments, kernel, **kwargs)\u001b[39m\n\u001b[32m    417\u001b[39m role = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    418\u001b[39m response_builder: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response_list \u001b[38;5;129;01min\u001b[39;00m responses:\n\u001b[32m    420\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m response_list:\n\u001b[32m    421\u001b[39m         role = response.role\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\chat_completion_client_base.py:261\u001b[39m, in \u001b[36mChatCompletionClientBase.get_streaming_chat_message_contents\u001b[39m\u001b[34m(self, chat_history, settings, **kwargs)\u001b[39m\n\u001b[32m    259\u001b[39m all_messages: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mStreamingChatMessageContent\u001b[39m\u001b[33m\"\u001b[39m] = []\n\u001b[32m    260\u001b[39m function_call_returned = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m messages \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inner_get_streaming_chat_message_contents(\n\u001b[32m    262\u001b[39m     chat_history, settings, request_index\n\u001b[32m    263\u001b[39m ):\n\u001b[32m    264\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m messages:\n\u001b[32m    265\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\semantic_kernel\\utils\\telemetry\\model_diagnostics\\decorators.py:165\u001b[39m, in \u001b[36mtrace_streaming_chat_completion.<locals>.inner_trace_streaming_chat_completion.<locals>.wrapper_decorator\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(completion_func)\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper_decorator\u001b[39m(\n\u001b[32m    161\u001b[39m     *args: Any, **kwargs: Any\n\u001b[32m    162\u001b[39m ) -> AsyncGenerator[\u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mStreamingChatMessageContent\u001b[39m\u001b[33m\"\u001b[39m], Any]:\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m are_model_diagnostics_enabled():\n\u001b[32m    164\u001b[39m         \u001b[38;5;66;03m# If model diagnostics are not enabled, just return the completion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m streaming_chat_message_contents \u001b[38;5;129;01min\u001b[39;00m completion_func(*args, **kwargs):\n\u001b[32m    166\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m streaming_chat_message_contents\n\u001b[32m    167\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_chat_completion_base.py:110\u001b[39m, in \u001b[36mOpenAIChatCompletionBase._inner_get_streaming_chat_message_contents\u001b[39m\u001b[34m(self, chat_history, settings, function_invoke_attempt)\u001b[39m\n\u001b[32m    107\u001b[39m settings.messages = \u001b[38;5;28mself\u001b[39m._prepare_chat_history_for_request(chat_history)\n\u001b[32m    108\u001b[39m settings.ai_model_id = settings.ai_model_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ai_model_id\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_request(settings)\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, AsyncStream):\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceInvalidResponseError(\u001b[33m\"\u001b[39m\u001b[33mExpected an AsyncStream[ChatCompletionChunk] response.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_handler.py:59\u001b[39m, in \u001b[36mOpenAIHandler._send_request\u001b[39m\u001b[34m(self, settings)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ai_model_type == OpenAIModelTypes.TEXT \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ai_model_type == OpenAIModelTypes.CHAT:\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(settings, OpenAIPromptExecutionSettings)  \u001b[38;5;66;03m# nosec\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_completion_request(settings)\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ai_model_type == OpenAIModelTypes.EMBEDDING:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(settings, OpenAIEmbeddingPromptExecutionSettings)  \u001b[38;5;66;03m# nosec\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GIT\\VitalMind\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_handler.py:104\u001b[39m, in \u001b[36mOpenAIHandler._send_completion_request\u001b[39m\u001b[34m(self, settings)\u001b[39m\n\u001b[32m     99\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceResponseException(\n\u001b[32m    100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m service failed to complete the prompt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    101\u001b[39m         ex,\n\u001b[32m    102\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceResponseException(\n\u001b[32m    105\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m service failed to complete the prompt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    106\u001b[39m         ex,\n\u001b[32m    107\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n",
      "\u001b[31mServiceResponseException\u001b[39m: (\"<class 'semantic_kernel.connectors.ai.open_ai.services.open_ai_chat_completion.OpenAIChatCompletion'> service failed to complete the prompt\", AuthenticationError(\"Error code: 401 - {'error': {'code': 'unauthorized', 'message': 'Bad credentials', 'details': 'Bad credentials'}}\"))"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "# Crear un nuevo hilo para el agente\n",
    "# Si no se proporciona ningún hilo, \n",
    "# se creará uno nuevo y se devolverá con la respuesta inicial.\n",
    "    thread: ChatHistoryAgentThread | None = None\n",
    "\n",
    "    user_inputs = [\n",
    "        \"Tengo 75 años y me han subido los niveles de glucosa, ¿qué puedo hacer?\",\n",
    "        \"Mi padre tiene 82 años y a veces se le olvida tomar sus medicamentos, ¿cómo puedo ayudarlo?\",\n",
    "        \"¿Qué debo hacer si tengo presión arterial alta y me siento mareado?\",\n",
    "    ]\n",
    "    for user_input in user_inputs:\n",
    "        print(f\"# User: {user_input}\\n\")\n",
    "        first_chunk = True\n",
    "        async for response in agent.invoke_stream(\n",
    "            messages=user_input, thread=thread,\n",
    "        ):\n",
    "            # 5. Print the response\n",
    "            if first_chunk:\n",
    "                print(f\"# {response.name}: \", end=\"\", flush=True)\n",
    "                first_chunk = False\n",
    "            print(f\"{response}\", end=\"\", flush=True)\n",
    "            thread = response.thread\n",
    "        print()\n",
    "\n",
    "    # Clean up the thread\n",
    "    await thread.delete() if thread else None\n",
    "\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
